# Core dependencies for GPT-OSS MoE-8B Pretraining

# PyTorch (CUDA 12.1 recommended)
torch>=2.1.0
torchvision>=0.16.0

# Tokenization
tiktoken>=0.5.0

# Experiment tracking
wandb>=0.16.0

# Configuration
pyyaml>=6.0
omegaconf>=2.3.0

# Utilities
numpy>=1.24.0
tqdm>=4.66.0

# Model export
safetensors>=0.4.0

# FlashAttention (install separately based on CUDA version)
pip install flash-attn --no-build-isolation
triton>=2.1.0  # Included with flash-attn

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Optional: For AWS cost tracking
# boto3>=1.28.0

# Optional: For distributed training helpers
# accelerate>=0.24.0
